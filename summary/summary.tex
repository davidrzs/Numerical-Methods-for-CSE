\documentclass{sciposter}
\usepackage[dvipsnames,usenames,svgnames,table,x11names, rgb, html]{xcolor} 
\usepackage{lipsum}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[german]{babel}
\usepackage{geometry}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{wrapfig}
\usepackage{gensymb}
\usepackage[utf8]{inputenc}
\usepackage{empheq}
\usepackage{mathpazo}
\usepackage[T1]{fontenc}
\renewcommand{\familydefault}{\rmdefault}
\usepackage{eulervm}

% so we have more space between matrix entries
\setlength{\arraycolsep}{12pt}

\geometry{
 landscape,
 a1paper,
 left=5mm,
 right=50mm,
 top=5mm,
 bottom=50mm,
 }


\usepackage{todonotes}

\newcommand{\nnz}{d\operatorname{nnz}}
\newcommand{\TODO}[1]{\todo[inline, color=red!40]{#1}}
\renewcommand{\vec}[1]{\mathbf{#1}}
%BEGIN LISTINGDEF



\usepackage{listings}
\usepackage{inconsolata}


\usepackage[framemethod=TikZ]{mdframed}
\newenvironment{method}[1]{\begin{mdframed}[backgroundcolor=blue!10,innertopmargin=15pt, innerbottommargin=15pt, nobreak=true]
		\textbf{#1 }
	}
	{ 
	\end{mdframed}
}


\definecolor{background}{HTML}{FAFAFA}
\definecolor{comment}{HTML}{ABB0B6}
\definecolor{keywords}{HTML}{55B4D4}
\definecolor{basicStyle}{HTML}{6C7680}
\definecolor{variable}{HTML}{001080}
\definecolor{string}{HTML}{86B300}


\lstset{
	% How/what to match
	sensitive=true,
	% Border (above and below)
	frame=single,
	% Extra margin on line (align with paragraph)
	xleftmargin=\parindent,
	% Put extra space under caption
	belowcaptionskip=1\baselineskip,
	% Colors
	backgroundcolor=\color{background},
	basicstyle=\color{basicStyle}\ttfamily,
	keywordstyle=\color{keywords},
	commentstyle=\color{comment},
	stringstyle=\color{string},
	numberstyle=\color{sviolet},
	identifierstyle=\color{variable},
	% Break long lines into multiple lines?
	breaklines=true,
	% Show a character for spaces?
	showstringspaces=false,
	tabsize=2
}

%END LISTINGDEF


\newcommand*\widefbox[1]{\fbox{\hspace{2em}#1\hspace{2em}}}


\newlength\dlf  % Define a new measure, dlf
\newcommand\alignedbox[2]{
% Argument #1 = before & if there were no box (lhs)
% Argument #2 = after & if there were no box (rhs)
&  % Alignment sign of the line
{
\settowidth\dlf{$\displaystyle #1$}
    % The width of \dlf is the width of the lhs, with a displaystyle font
\addtolength\dlf{\fboxsep+\fboxrule}
    % Add to it the distance to the box, and the width of the line of the box
\hspace{-\dlf}
    % Move everything dlf units to the left, so that & #1 #2 is aligned under #1 & #2
\boxed{#1 #2}
    % Put a box around lhs and rhs
}
}
\usepackage{graphicx,url}

%BEGIN TITLE
\title{\huge{Numerical Methods for CSE}}

\author{\large{David Zollikofer}}
%END TITLE


\newcommand{\psection}[1]{\par \textbf{\large#1}}

\begin{document}
%\fontfamily{phv}\selectfont

\maketitle



\begin{multicols}{2}
\section{Eigen \& C++}

\psection{Constructing Matrices and Vectors}
\begin{lstlisting}[language=C++]
// same data different shape
Map<Matrix<TYPE, Dynamic , Dynamic>> newMat(M.data(),newRows,newCols);
// (Deep) copy data of M
MatrixXd xMat = MatrixXd::Map(M.data(), n, n);
\end{lstlisting}


\psection{Sizes, Rows, Cols, Resize}
\begin{lstlisting}[language=C++]
A.size() // total number of elements in matrix
A.rows() // number of rows
A.cols() // number of cols
A.resize(10,100) // resized to 10 rows and 100 columns
\end{lstlisting}


\psection{Filling matrices}
\begin{lstlisting}[language=C++]
A << R,	v // can also be done using blocks. 
u .transpose( ) ,  0;
B << A, A, A;     // B is three horizontally stacked A's.
A.fill(10);       // Fill A with all 10's.

MatrixXd::Identity(rows,cols)          
C.setIdentity(rows,cols)                 
MatrixXd::Zero(rows,cols)                
C.setZero(rows,cols)                      
MatrixXd::Ones(rows,cols)          
C.setOnes(rows,cols)                 
MatrixXd::Random(rows,cols)  // uniform random numbers in (-1,1).
C.setRandom(rows,cols)                 
VectorXd::LinSpaced(size,low,high)       // includes lower and higher bound -> if size=1 only higher. 
v.setLinSpaced(size,low,high)         
VectorXi::LinSpaced(((hi-low)/step)+1, low,low+step*(size-1))
\end{lstlisting}



\psection{Reshaping using Map}
\begin{lstlisting}[language=C++]
// same data different shape
Map<Matrix<TYPE, Dynamic , Dynamic>> newMat(M.data(),newRows,newCols);
// (Deep) copy data of M
MatrixXd xMat = MatrixXd::Map(M.data(), n, n);
\end{lstlisting}


\psection{Solving Equations}
\begin{lstlisting}[language=C++]
// Solve Ax = b. Result stored in x
x = A.ldlt().solve(b));  // A sym. p.s.d.    #include <Eigen/Cholesky>
x = A.llt() .solve(b));  // A sym. p.d.      #include <Eigen/Cholesky>
x = A.lu()  .solve(b));  // Stable and fast. #include <Eigen/LU>
x = A.qr()  .solve(b));  // No pivoting.     #include <Eigen/QR>
x = A.svd() .solve(b));  // Stable, slowest. #include <Eigen/SVD>
// .ldlt() -> .matrixL() and .matrixD()
// .llt()  -> .matrixL()
// .lu()   -> .matrixL() and .matrixU()
// .qr()   -> .matrixQ() and .matrixR()
// .svd()  -> .matrixU(), .singularValues(), and .matrixV()
\end{lstlisting}

\psection{Triangular Views}
We have the following options \texttt{Upper, Lower, StrictlyUpper, StrictlyLower, UnitUpper, UnitLower}
\begin{lstlisting}
	MatrixXd m2 = m.triangularView<Xxx>();
\end{lstlisting}

\psection{Reductions}
\begin{lstlisting}[language=C++]
R.minCoeff()              
R.maxCoeff()              
s = R.minCoeff(&r, &c)   
R.sum()                  
R.colwise().sum()      
R.prod()                
R.rowwise().prod()        
R.trace()                
(a > 2).all() // if all a_i > 2 true             
R.rowwise().all()     
(a > 2).any()    // if any a_i > 2 true        
(a > 2).count() // count a_i > 2 
R.colwise().any()  
x.norm()                 
x.squaredNorm()       
x.dot(y)                 
x.cross(y)     //Requires #include <Eigen/Geometry>
\end{lstlisting}


\TODO{triangulare view etc, diagonal etc .}

\TODO{sparse formats}
\TODO{CRS / COO format}

\TODO{TEMPLATE STUFF}
\TODO{constructor in C++}


\TODO{linspaced how to use}

\TODO{timing 6.1 interpol -> see timing}


\psection{Kronecker Product}
\begin{lstlisting}
	C = kroneckerProduct(A,B).eval();
\end{lstlisting}


\section{Sparse Matrices}
\TODO{add matrix multiplication formula using sums}
\TODO{cholesky decomposition}

\TODO{nnz arithmetics}
\psection{NNZ Arithmetics}
Es gilt:


\TODO{unary expression}

\psection{Defining sparse matrices}
\begin{lstlisting}[language=C++]
#include <Eigen/Sparse>
SparseMatrix<double> mat(rows,cols);

std::vector<Triplet<double>> tripletVec;
tripletVec.reserve(n);
for(int i = 0; i < n; i++){
double val = 2;
Triplet<double> newTriplet(row,col,val);
tripletVec.push_back(newTriplet);
}
X.setFromTriplets(tripletVec.begin(), tripletVec.end());
X.makeCompressed();
\end{lstlisting}



\psection{Solving Linear Equations with Sparse Matrices}
\begin{lstlisting}[language=C++]
Eigen::SparseMatrix<double> A(n,n);
A.reserve(3*n);
for(;;){
	A.insert(i,i-1) = 100;
}
A.makeCompressed(); // VERY IMPORTANT
Eigen::SparseLU<Eigen::SparseMatrix<double>> Ax_lu(A) 
newX = Ax_lu .solve(b);
\end{lstlisting}


\psection{Using sparse matrixes}
\begin{lstlisting}[language=c++]
	//now we loop over every non zero entry of M (for the left hand size of the kronecker product)
	// loop over every column
	for(int i = 0; i < n;i++){
		// we loop as long as there are still elements in the column
		// we get the pointer to where the column starts and loop for as long as elements still are in this column
		for(int index = colPointer[i]; index < colPointer[i+1] ; index++){
			// we are now at the nonzero element in row "i" and column "j" and value "outerValue".
			int outerRow = rowPointer[index];
			int outerColumn = i;
			double outerValue = values[index];
			
			// now we need to loop over all of M again to add the block to the kronecker matrix
			
			for(int innerI = 0; innerI < n;innerI++){
				// we loop as long as there are still elements in the row
				for(int innerIndex = colPointer[innerI]; innerIndex < colPointer[innerI+1] ; innerIndex++){
					
					int innerRow = rowPointer[innerIndex];
					int innerColumn = innerI;
					double innerValue = values[innerIndex];
					
					
					Triplet<double> toAdd(n*outerRow+innerRow,n*outerColumn+innerColumn,outerValue*innerValue);
					bVec.push_back(toAdd);
				}
			}
		}
	}
\end{lstlisting}

\section{QR Decompositions}
\psection{Givens Rotation}
See \texttt{3.3.3.14} as well as \texttt{3.3.3.20}

\TODO{add different mehods}



\section{Least Squares}

\psection{Solving Least Squares Problems}
\begin{lstlisting}[language=c++]
JacobiSVD<MatrixXd> svd(A,ComputeThinU | ComputeThinV);
VectorXd solution = svd.solve(b);
\end{lstlisting}
\psection{Constrained Least Squares (3.6)}
Assume we want to minimize $|| Ax-b ||_2$ under the constraint $Cx = d$. We use Lagrangian multipliers and get the \textit{augmented normal equations}:
\begin{align*}
	  \begin{bmatrix}
	\vec{A}^T\vec{A} & \vec{C}^T \\ 
\vec{C} & 0 
	\end{bmatrix} \begin{bmatrix}
	\vec{x} \\ \vec{m} 
	\end{bmatrix} &= \begin{bmatrix}
	\vec{A}^T \vec{b}\\ \vec{d}
	\end{bmatrix}
\end{align*}
Alternatively we can also use the extended normal equations and get
\begin{align*}
\begin{bmatrix}
\vec{-I} & \vec{A} & 0\\
\vec{A}^T & 0 & \vec{C}^T\\
0 & \vec{C} & 0 
\end{bmatrix}
\begin{bmatrix}
\vec{r} \\ \vec{x} \\ \vec{m} 
\end{bmatrix} &=
\begin{bmatrix}
\vec{b}\\ 0 \\ \vec{d}
\end{bmatrix}
\end{align*}


\section{Gaussian Block Elimination}
Example:\begin{align*}
\begin{bmatrix}
\vec{\mathbb{1}} & \vec{C^T} \\
\vec{C} & \vec{0}
\end{bmatrix} \cdot \begin{bmatrix}
\vec{x} \\ \vec{m}
\end{bmatrix} &= \begin{bmatrix}
\vec{0} \\ \vec{g}
\end{bmatrix}\\
\Rightarrow \vec{1\cdot x} + \vec{C^T\cdot m} &= \vec{0} \Rightarrow -\vec{C^T}\vec{m} = \vec{x}\\
\Rightarrow \vec{C x} &= \vec{g} \\
\Rightarrow \vec{-C}\cdot( \vec{C^Tm}) &= \vec{g}
\end{align*}

\section{Singular Value Decompostion}
low rank rep





\section{Data Interpolation and Fitting (Chapter 5)}


There are three main approaches:

\begin{itemize}
	\item Using Vandermonde Matrix $\rightarrow$ never do this. Very badly conditioned matrix.
	\item \textbf{Lagrange} Good choice, use Barycentric if interested in coeffs, if only need one eval use Aitken-Neville.
	\item \textbf{Newton} If we dynamically add new data (update friendly) (5.2.3.23)
\end{itemize}


\textbf{Extraplation to 0}: Works really well if $\phi(t) = \phi(-t)$, we use Aitken-Neville on extrapolation point.


\TODO{local interpolation}




\section{Function Approximation (Chapter 6)}

Approximation is the combination of an interpolation scheme and sampling. We get to choose where to evaluate the function $f$.

\begin{method}{Jackson's Theorem (6.1.1.11)}
	If $f\in C^r([-1,1])$ ($r\in \mathbb{N}$ times continuously differentiable), then for any polynomial of degree $n \geq r$
	\begin{align*}
		\inf_{p \in \mathcal{P}} || f-p ||_{L^\infty [(-1,1)]} \leq (1 + \frac{\pi^2}{2})^r \frac{(n-r)!}{n!} || f^{(r)} ||_{L^\infty [(-1,1)]}
	\end{align*}
\end{method}

We can also apply Jackson's Theorem to any interval using Lemma 6.1.1.20



\psection{Convergence}


\begin{itemize}
\item Algebraic Convergence: data on line in log-log plot, $\exists p > 0:$  $T(n) \leq n^{-p}$

\item Exponential Convergence: data on line in lin-log plot , $\exists 0 < q < 1:$  $T(n) \leq q^n$

\end{itemize}




\section{Quadrature (Chapter 7)}


Basic Idea. We would like to approximate $\int_{a}^{b} f(x) dx$. 

For this we use a quadrature rule \TODO{add stuff}


\newpage

\section{Direct Methods for Linear Least Squares}

\section{Filtering Algorithms}

\section{Data Interpolation and Data Fitting in 1D}

\section{Approximating Functions in 1D}

\section{Numerical Quadrature}

\section{Iterative Methods for Non-Linear Systems of Equations}


\section{Numerical Integration - Single Step Methods}

\section{Single Methods for Stiff Initial Value Problems}


	

\section*{Lambda Functions}
\psection{Lambda Functions \& Applying Coefficient wise}
\begin{lstlisting}[language=C++]
auto phi1 = [](int t) -> double {return 1/t;};
auto phi2 = [](int t){return 1/(t*t);};
b = b.unaryExpr(phi1);
\end{lstlisting}


\begin{lstlisting}[language=c++]
#include <functional>
int main(){
	std::function<double(int)>  f = [](int l){
		return l + 3.14;
	};
	std::cout << f(2);
}
\end{lstlisting}





\section*{Using \texttt{std::vector<...>}}


\psection{Turn \texttt{std::vector<...>} to \texttt{VectorXd}}

\begin{lstlisting}[language=c++]
std: :vector<double> t {12};
Map<VectorXd> te (t.data(),t.size());
\end{lstlisting}



https://stackoverflow.com/questions/46809901/lambda-captures


add integration rules -> substitution



\section{Mathplotlib Plotting}
% matplotlibcpp
\begin{lstlisting}
VectorXd ,tf2;// nodes for f1 resp. f
std::vector<double> ef1 , ef2 ;// errors for f1 resp. f2
tf1 = adaptivepolyintp(f1,a,b,tol,N,&ef1);
tf2 = adaptivepolyintp(f2,a,b,tol,N,&ef2);
VectorXd n1 = VectorXd::LinSpaced(ef1.size(),1,ef1.size());
VectorXd n2 = VectorXd::LinSpaced(ef2.size(),1,ef2.size());
plt::figure();
plt::title("Error VS step") ;
plt::xlabel("No. of interpolation nodes") ;
plt::ylabel("sdsd");
plt::semilogy(n1,ef1,"ro",{ {" label","f1(t) =sin(e2t)"}});
plt::semilogy(n2,ef2,"bo",{{"label","f2(t)=sqrtt/(1+16t2)"}});
plt::legend();
plt::savefig("./cx_out/intperrplot.png");
\end{lstlisting}





\section{Differential Equations}

\psection{Transforming to autonomous 1st order ODE}

Assume we have $y'' = -g\sin(y)$, we define $u_1 = y$ and $u_2 = y'$. This yields:
\begin{align*}
	\vec{u} &= \begin{pmatrix}
	u_1 \\ u_2
	\end{pmatrix} = \begin{pmatrix}
	y \\ y'
	\end{pmatrix} & f(t,\vec{u}) = \begin{pmatrix}
	u_2 \\ -g\sin(u_1)
	\end{pmatrix} = \begin{pmatrix}
	u_1 '\\ u_2 '
	\end{pmatrix} = \vec{u}'
\end{align*}


We can also get rid of a dependence on $t$ (make autonomous): 

\psection{Runge Kutta Methods}

We have the IVP  $\dot{\vec{y}} = \vec{f}(t,\vec{y})$, $\vec{y}(t_0) = \vec{y_0}$. We can write $\vec{y_1}$ as $$\vec{y}(t_1) = \vec{y_0} + \int_{t_0}^{t_1} \vec{f}(\tau, \vec{y}(\tau)) d\tau $$
giving us
$$
\vec{y}(t_1) \approx \vec{y_0} + h\sum_{i=1}^{s} b_i \vec{f}(t_0 + c_i h , \vec{y}(t_0 + c_i h))
$$

\psection{Butcher Scheme}
We call a single step Runge Kutta method consistent iff. 
\begin{align*}
	\begin{array}
	{c|ccccc}
	\vec{c} &  \Theta \\
	\hline
	& \vec{b}^T
	\end{array} &:= 
	\begin{array}{c|cccc}
	c_1 &  0 & \ldots & \ldots & 0  \\
	c_2 &  a_{21} & \ddots &\ddots & \vdots\\
	\vdots & \vdots& \ddots & \ddots & \vdots \\
	c_s & a_{s1} & \ldots & a_{s,s_1} & 0\\
	\hline
	& b_1 &\ldots & \ldots b_{s-1} & b_s
	\end{array} 
\end{align*}

\psection{Find Order of Convergence of Runge-Kutta}
\begin{lstlisting}[language=c++]
VectorXd coeffs = polyfit(nArr.array().log(), err.array().log(),1); 
conv_rate = -coeffs(0);
\end{lstlisting}










\end{multicols}
\end{document}